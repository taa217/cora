# Voice Companion - Real-Time Voice Chat

A production-ready React + TypeScript application for low-latency, hands-free conversations powered by DeepInfra's `sesame/csm-1b` model.

## Features

- ðŸŽ¤ **Live voice capture** with pause/resume controls (MediaRecorder API)
- ðŸ“ **Instant speech-to-text** via the browser Web Speech API
- ðŸ¤– **Streaming DeepInfra replies** using the OpenAI-compatible `/chat/completions` endpoint
- ðŸ”Š **Automatic spoken responses** through the Web Speech synthesis engine
- ðŸ’¬ **Conversation timeline** with context memory (last ~8 turns)
- ðŸŽ¨ **Glassmorphism UI** tuned for desktop and touch devices
- âš¡ **Vite + React + Tailwind** for production-grade performance

## Tech Stack

- **React 18** with hooks
- **TypeScript 5**
- **Vite 5**
- **Tailwind CSS 3**
- **Web Speech + MediaRecorder APIs**
- **DeepInfra OpenAI-compatible API** (`sesame/csm-1b`)

## Getting Started

### Prerequisites

- Node.js 18+ and npm
- A DeepInfra account and API key

### Installation

1. Install dependencies
   ```bash
   npm install
   ```
2. Copy the environment template and add your keys + model ids
   ```bash
   cp .env.example .env
   # edit .env and set VITE_DEEPINFRA_API_KEY, VITE_DEEPINFRA_MODEL_ID, VITE_OPENAI_API_KEY, VITE_OPENAI_MODEL
   ```
3. Start the dev server
   ```bash
   npm run dev
   ```
4. Open `http://localhost:3000`

### Build & Preview

```bash
npm run build
npm run preview
```

## Environment Variables

| Name | Description |
| --- | --- |
| `VITE_DEEPINFRA_API_KEY` | DeepInfra API key with access to Sesame CSM |
| `VITE_DEEPINFRA_MODEL_ID` | Exact TTS model slug (see instructions below) |
| `VITE_OPENAI_API_KEY` | OpenAI key for text reasoning |
| `VITE_OPENAI_MODEL` | Chat model (e.g. `gpt-4o-mini`) |

> Keep secrets out of version control. Use `.env` locally and hosting-provider secrets in deployment.

**Finding the Sesame model slug**

DeepInfra exposes a model listing over the OpenAI-compatible API. After setting your API key, run:

```bash
curl https://api.deepinfra.com/v1/openai/models \
  -H "Authorization: Bearer $VITE_DEEPINFRA_API_KEY"
```

Set `VITE_DEEPINFRA_MODEL_ID` to the `id` value that corresponds to the Sesame Conversational Speech Model (for example `sesame/csm-1b` as shown on [DeepInfra](https://deepinfra.com/sesame/csm-1b)).

**Choosing the OpenAI model**

Use `curl https://api.openai.com/v1/models -H "Authorization: Bearer $VITE_OPENAI_API_KEY"` to list the models available to your account, then reference the `id` in `VITE_OPENAI_MODEL`.

## Real-Time Voice Loop

1. Capture mic audio and stream interim transcripts with the Web Speech API.
2. Send transcripts + rolling context to DeepInfra with `{ stream: true }`.
3. Render incoming tokens immediately for sub-second perceived latency.
4. Send the completed reply to DeepInfraâ€™s Sesame CSM endpoint and play the returned audio blob.

## Latency Tips

- Keep user prompts short; the last ~8 turns are sent to DeepInfra.
- Deploy the frontend close to DeepInfra regions.
- Prefer wired mics or high-quality Bluetooth profiles to reduce capture lag.

## DeepInfra Reference Links

- [Sesame CSM-1B model card](https://deepinfra.com/sesame/csm-1b)
- [OpenAI-compatible streaming docs](https://deepinfra.com/docs/deep_infra_api#tag/OpenAI-Compatible)

## Testing Checklist

- âœ… Microphone permission prompt and recording controls function correctly.
- âœ… Transcript updates in near real-time while speaking.
- âœ… "Send to Companion" streams OpenAI tokens with no console errors.
- âœ… Replies are played using the Sesame CSM audio returned by DeepInfra (falls back to native speech synthesis if the API call fails).
- âœ… Reset clears conversation context and aborts inflight requests.

## Project Structure

```
src/
â”œâ”€â”€ components/
â”‚   â””â”€â”€ VoiceRecorder/
â”‚       â”œâ”€â”€ VoiceRecorder.tsx        # Main container
â”‚       â”œâ”€â”€ ConversationPanel.tsx    # Chat timeline
â”‚       â”œâ”€â”€ MessageBubble.tsx        # Bubble UI
â”‚       â”œâ”€â”€ RecordingButton.tsx      # Capture controls
â”‚       â”œâ”€â”€ TranscriptDisplay.tsx    # Live transcript
â”‚       â”œâ”€â”€ AudioPlayer.tsx          # Playback widget
â”‚       â””â”€â”€ ErrorMessage.tsx         # Alert banner
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useVoiceRecorder.ts          # Mic + transcription
â”‚   â””â”€â”€ useVoiceAssistant.ts         # DeepInfra + chat loop
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ deepinfra.ts                 # Streaming client
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ speech.ts                    # Speech synthesis helpers
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ voice.ts                     # Voice + chat types
â”‚   â””â”€â”€ speech.d.ts                  # Web Speech typings
â”œâ”€â”€ App.tsx                          # Root component
â”œâ”€â”€ main.tsx                         # Entry point
â””â”€â”€ index.css                        # Global styles
```

## Browser Compatibility

- **Chrome / Edge**: Full support (recommended)
- **Firefox**: Works, but speech recognition can vary by locale
- **Safari**: Limited (Web Speech API still experimental)
- **Mobile Chrome / Edge**: Supported with HTTPS and user gestures

## License

MIT
